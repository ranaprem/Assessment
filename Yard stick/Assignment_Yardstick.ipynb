{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10080,
     "status": "ok",
     "timestamp": 1757755001734,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "eW94c6U9W4vw",
    "outputId": "11a517e2-10a7-4b2f-dc9b-62edb0be6def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1757755210922,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "zkWvhp6TXPDK"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757755212697,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "k-NMuO8OXHeg"
   },
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141930,
     "status": "ok",
     "timestamp": 1757755424930,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "sHbqThIOX0fU",
    "outputId": "a420e757-f3a2-410b-988d-f6d96efdd368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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··········\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = getpass(\"AIP KEY \")\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "error",
     "timestamp": 1757755477863,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "gD43C8bnX0gM",
    "outputId": "b3a8c2d2-ef75-4f80-a2dc-9edf37098ad8"
   },
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3934225336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-8b-8192\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Hello, can you confirm my API key works?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, can you confirm my API key works?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 3684,
     "status": "error",
     "timestamp": 1757755568386,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "GAoNr5OgX0kU",
    "outputId": "f5db89c2-055d-46a7-cfdd-066707abb29d"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1099993743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.groq.com/openai/v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-8b-8192\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Hello, can you confirm my API key works?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai.api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, can you confirm my API key works?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1757755708393,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "SlF4h_vPX0ll",
    "outputId": "947ac066-20a2-4c6c-bb9c-193c0ae33dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This conversation just started, and I'm a large language model, I don't have the ability to store or access your API key directly. I'm also not aware of any specific API key you might be referring to.\n",
      "\n",
      "However, if you'd like to test your API key with me, you can try using it to authenticate with a specific API or service that I can interact with. Please let me know what kind of API key it is and how I can help you verify it. I can try to provide a test or example to help you check if your API key is working as expected.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai.api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, can you confirm my API key works?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1757756567457,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "x4BAcTfRdKF-",
    "outputId": "055b749a-5c3d-4d75-d0d1-edad0f06c626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'message': 'Hello, can you confirm my API key works?'}, {'role': 'assistant', 'message': 'Yes, your API key works perfectly!'}]\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def add_message(role, message):\n",
    "    conversation_history.append({\"role\": role, \"message\": message})\n",
    "\n",
    "add_message(\"user\", \"Hello, can you confirm my API key works?\")\n",
    "add_message(\"assistant\", \"Yes, your API key works perfectly!\")\n",
    "\n",
    "print(conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757757283650,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "yFA75pVgZ45E"
   },
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def add_message(role: str, content: str):\n",
    "    assert role in (\"user\", \"assistant\", \"system\"), \"role must be user|assistant|system\"\n",
    "    assert isinstance(content, str) and len(content) > 0, \"content must be non-empty string\"\n",
    "    conversation_history.append({\"role\": role, \"content\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1757757317537,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "uF-dxyG5f7EZ"
   },
   "outputs": [],
   "source": [
    "def truncate_by_last_n(messages: list[dict], n: int | None):\n",
    "    \"\"\"\n",
    "    Keep only the last n messages. If n is None, return all messages.\n",
    "    Each list item is an OpenAI-format message: {\"role\": ..., \"content\": ...}\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        return messages\n",
    "    return messages[-n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757757352332,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "qtWBE-vNf7AU"
   },
   "outputs": [],
   "source": [
    "def truncate_by_char_limit(messages: list[dict], max_chars: int | None):\n",
    "    \"\"\"\n",
    "    Keep as many of the most recent messages as will fit within max_chars\n",
    "    when summing len(content). If max_chars is None, return all messages.\n",
    "    \"\"\"\n",
    "    if max_chars is None:\n",
    "        return messages\n",
    "\n",
    "    total = 0\n",
    "    kept_reversed = []\n",
    "\n",
    "    for msg in reversed(messages):\n",
    "        c = len(msg.get(\"content\", \"\") or \"\")\n",
    "        if total + c > max_chars:\n",
    "            break\n",
    "        kept_reversed.append(msg)\n",
    "        total += c\n",
    "\n",
    "    return list(reversed(kept_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757757392720,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "SfS22cSZf694"
   },
   "outputs": [],
   "source": [
    "def apply_truncation(messages: list[dict], last_n: int | None = None, max_chars: int | None = None):\n",
    "    \"\"\"\n",
    "    Apply character-limit first (to keep newest messages within size),\n",
    "    then apply last-N to ensure a hard cap on message count if desired.\n",
    "    \"\"\"\n",
    "    msgs = truncate_by_char_limit(messages, max_chars=max_chars)\n",
    "    msgs = truncate_by_last_n(msgs, n=last_n)\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1757757438379,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "VUcrC7xWf68C"
   },
   "outputs": [],
   "source": [
    "def send_chat(\n",
    "    client,\n",
    "    model: str,\n",
    "    user_text: str,\n",
    "    last_n: int | None = None,\n",
    "    max_chars: int | None = None,\n",
    "    temperature: float = 0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Appends the user_text to the full conversation_history\n",
    "    - Builds a truncated context using apply_truncation()\n",
    "    - Calls chat.completions.create() on the Groq OpenAI-compatible endpoint\n",
    "    - Appends the assistant reply back into full conversation_history\n",
    "    - Returns the assistant reply text\n",
    "    \"\"\"\n",
    "    add_message(\"user\", user_text)\n",
    "\n",
    "    context = apply_truncation(conversation_history, last_n=last_n, max_chars=max_chars)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=context,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    reply = resp.choices.message.content\n",
    "    add_message(\"assistant\", reply)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1757757639955,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "_XM-X5egf65s"
   },
   "outputs": [],
   "source": [
    "def summarize_history(client, model: str, messages_slice: list[dict], max_words: int = 120) -> str:\n",
    "    \"\"\"\n",
    "    Calls the model to summarize the provided messages_slice.\n",
    "    Returns a concise summary string with key intents, decisions, and open questions.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a conversation summarizer. \"\n",
    "        f\"Summarize the chat succinctly in <= {max_words} words. \"\n",
    "        \"Capture: user goals, assistant guidance, key decisions, and open questions.\"\n",
    "    )\n",
    "\n",
    "    # Prepare a self-contained input: system + the slice as a compact text block\n",
    "    # We stringify the slice so the model sees exactly what to summarize.\n",
    "    slice_text = \"\\n\".join(f\"{m['role']}: {m.get('content','')}\" for m in messages_slice)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following dialogue:\\n\\n{slice_text}\"},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return resp.choices.message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1757757666963,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "5IH5G0jIf63M"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compact_history_with_summary(\n",
    "    client,\n",
    "    model: str,\n",
    "    keep_last: int = 6,\n",
    "    max_words: int = 120,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Summarizes all but the most recent `keep_last` messages.\n",
    "    - Replaces that older block with a single assistant summary message.\n",
    "    - Leaves the last `keep_last` messages verbatim.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    if len(conversation_history) <= keep_last:\n",
    "        return  # nothing to compact\n",
    "\n",
    "    older = conversation_history[:-keep_last]\n",
    "    recent = conversation_history[-keep_last:]\n",
    "\n",
    "    summary_text = summarize_history(client, model, older, max_words=max_words)\n",
    "\n",
    "    summary_msg = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"(Summary of earlier context) {summary_text}\"\n",
    "    }\n",
    "\n",
    "    conversation_history = [summary_msg] + recent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1757757711529,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "i9lXvdXYf60e"
   },
   "outputs": [],
   "source": [
    "turn_count = 0\n",
    "\n",
    "def send_chat_with_periodic_summary(\n",
    "    client,\n",
    "    model: str,\n",
    "    user_text: str,\n",
    "    k: int = 3,\n",
    "    keep_last: int = 6,\n",
    "    last_n: int | None = None,\n",
    "    max_chars: int | None = None,\n",
    "    temperature: float = 0.3,\n",
    "    summary_max_words: int = 120,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Sends a chat message via send_chat (which applies truncation).\n",
    "    - After every k-th user turn, compacts earlier history into one summary,\n",
    "      keeping the most recent `keep_last` messages verbatim.\n",
    "    \"\"\"\n",
    "    global turn_count\n",
    "\n",
    "    reply = send_chat(\n",
    "        client=client,\n",
    "        model=model,\n",
    "        user_text=user_text,\n",
    "        last_n=last_n,\n",
    "        max_chars=max_chars,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    turn_count += 1\n",
    "    if k is not None and k > 0 and (turn_count % k == 0):\n",
    "        compact_history_with_summary(\n",
    "            client=client,\n",
    "            model=model,\n",
    "            keep_last=keep_last,\n",
    "            max_words=summary_max_words,\n",
    "        )\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 1201,
     "status": "error",
     "timestamp": 1757757954872,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "NfHucJx8f6x2",
    "outputId": "44747d9f-8889-4e40-df80-a082ba0463f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User turn 1 ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1811122721.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- User turn {i} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     reply = send_chat_with_periodic_summary(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2845585892.py\u001b[0m in \u001b[0;36msend_chat_with_periodic_summary\u001b[0;34m(client, model, user_text, k, keep_last, last_n, max_chars, temperature, summary_max_words)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Send normally with truncation controls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     reply = send_chat(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2721711793.py\u001b[0m in \u001b[0;36msend_chat\u001b[0;34m(client, model, user_text, last_n, max_chars, temperature)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 4) read assistant reply and store to full history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m  \u001b[0;31m# <-- fixed indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0madd_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "conversation_history.clear()\n",
    "turn_count = 0\n",
    "\n",
    "model_id = \"llama-3.1-8b-instant\"\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    \"Hi, I want help planning a study schedule for Oracle Gen AI exam.\",\n",
    "    \"I can study 2 hours daily. Prefer mornings.\",\n",
    "    \"Please suggest a 2-week plan with milestones.\",\n",
    "    \"Also add quick review checkpoints every 3 days.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(inputs, start=1):\n",
    "    print(f\"\\n--- User turn {i} ---\")\n",
    "    reply = send_chat_with_periodic_summary(\n",
    "        client=client,\n",
    "        model=model_id,\n",
    "        user_text=text,\n",
    "        k=3,\n",
    "        keep_last=6,\n",
    "        last_n=12,\n",
    "        max_chars=4000,\n",
    "        summary_max_words=80,\n",
    "    )\n",
    "    print(\"Assistant:\", reply)\n",
    "\n",
    "\n",
    "    roles = [m[\"role\"] for m in conversation_history]\n",
    "    print(\"History size:\", len(conversation_history), \"| roles:\", roles)\n",
    "\n",
    "print(\"\\n=== Final conversation history ===\")\n",
    "for m in conversation_history:\n",
    "    print(f\"{m['role']}: {m['content'][:120]}{'...' if len(m['content'])>120 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757758042909,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "SFn-5drRf6vU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def send_chat(\n",
    "    client,\n",
    "    model: str,\n",
    "    user_text: str,\n",
    "    last_n: int | None = None,\n",
    "    max_chars: int | None = None,\n",
    "    temperature: float = 0.3,\n",
    "):\n",
    "    add_message(\"user\", user_text)\n",
    "\n",
    "    context = apply_truncation(conversation_history, last_n=last_n, max_chars=max_chars)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=context,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    reply = resp.choices[0].message.content  # <-- fixed indexing\n",
    "    add_message(\"assistant\", reply)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5130,
     "status": "ok",
     "timestamp": 1757758048721,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "zyH4NURaf6ss",
    "outputId": "d84c6ca1-e653-4af3-e1f0-4695e3a98dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User turn 1 ---\n",
      "Assistant: Here's a basic design for a small FAQ bot for a college site:\n",
      "\n",
      "**Bot Name:** CollegePal\n",
      "\n",
      "**Functionality:**\n",
      "\n",
      "1. **User Input:** The bot will accept user input in the form of questions or keywords related to college life.\n",
      "2. **Question Matching:** The bot will match the user's input with a predefined list of FAQs.\n",
      "3. **Answer Retrieval:** The bot will retrieve the corresponding answer from the FAQ database.\n",
      "4. **Response Generation:** The bot will generate a response to the user, including the answer and any relevant additional information.\n",
      "\n",
      "**Design Requirements:**\n",
      "\n",
      "1. **Natural Language Processing (NLP):** The bot will use NLP to understand the user's input and match it with the FAQs.\n",
      "2. **FAQ Database:** The bot will have a database of FAQs, which will be populated with common questions and answers related to college life.\n",
      "3. **User Interface:** The bot will have a simple user interface, such as a chat window or a web form, where users can input their questions.\n",
      "\n",
      "**Technical Requirements:**\n",
      "\n",
      "1. **Programming Language:** The bot will be built using a programming language such as Python or Java.\n",
      "2. **NLP Library:** The bot will use an NLP library such as NLTK or spaCy to process user input.\n",
      "3. **Database:** The bot will use a database such as MySQL or MongoDB to store FAQs.\n",
      "4. **Web Framework:** The bot will use a web framework such as Flask or Django to handle user input and generate responses.\n",
      "\n",
      "**Example FAQs:**\n",
      "\n",
      "1. Q: What are the office hours for the registrar's office?\n",
      "A: The registrar's office is open Monday through Friday from 9am to 5pm.\n",
      "2. Q: How do I register for classes?\n",
      "A: To register for classes, log in to your student portal and follow the instructions.\n",
      "3. Q: What are the deadlines for financial aid applications?\n",
      "A: The deadlines for financial aid applications are March 1st for the upcoming academic year.\n",
      "\n",
      "**Code Example:**\n",
      "\n",
      "Here's a simple example of how the bot might be implemented using Python and the NLTK library:\n",
      "```python\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "# Load FAQs from database\n",
      "faqs = [\n",
      "    {\"question\": \"What are the office hours for the registrar's office?\", \"answer\": \"The registrar's office is open Monday through Friday from 9am to 5pm.\"},\n",
      "    {\"question\": \"How do I register for classes?\", \"answer\": \"To register for classes, log in to your student portal and follow the instructions.\"},\n",
      "    {\"question\": \"What are the deadlines for financial aid applications?\", \"answer\": \"The deadlines for financial aid applications are March 1st for the upcoming academic year.\"}\n",
      "]\n",
      "\n",
      "# Define NLP function to process user input\n",
      "def process_input(user_input):\n",
      "    tokens = word_tokenize(user_input)\n",
      "    stop_words = set(stopwords.words(\"english\"))\n",
      "    tokens = [t for t in tokens if t not in stop_words]\n",
      "    lemmatizer = WordNetLemmatizer()\n",
      "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "# Define function to match user input with FAQs\n",
      "def match_input(user_input):\n",
      "    tokens = process_input(user_input)\n",
      "    for faq in faqs:\n",
      "        faq_tokens = process_input(faq[\"question\"])\n",
      "        if set(tokens).issubset(set(faq_tokens)):\n",
      "            return faq[\"answer\"]\n",
      "    return \"Sorry, I couldn't find an answer to your question.\"\n",
      "\n",
      "# Define function to generate response\n",
      "def generate_response(user_input):\n",
      "    answer = match_input(user_input)\n",
      "    return f\"Q: {user_input}\\nA: {answer}\"\n",
      "\n",
      "# Test the bot\n",
      "user_input = \"What are the office hours for the registrar's office?\"\n",
      "print(generate_response(user_input))\n",
      "```\n",
      "This is just a simple example, and there are many ways to improve the bot's functionality and accuracy.\n",
      "\n",
      "--- User turn 2 ---\n",
      "Assistant: With a CSV of Q&A and a PDF brochure, we can design a more robust FAQ bot. Here's an updated design:\n",
      "\n",
      "**Data Sources:**\n",
      "\n",
      "1. **CSV of Q&A:** This will be the primary source of FAQs, with each row representing a question and answer pair.\n",
      "2. **PDF Brochure:** This will be used to extract additional FAQs, such as those related to campus events, facilities, or services.\n",
      "\n",
      "**Data Preprocessing:**\n",
      "\n",
      "1. **CSV Processing:** We'll use a library like pandas to read the CSV file and preprocess the data. This will involve:\n",
      "\t* Tokenizing the questions and answers\n",
      "\t* Removing stop words and punctuation\n",
      "\t* Lemmatizing the tokens\n",
      "\t* Creating a dictionary to map question tokens to answer IDs\n",
      "2. **PDF Processing:** We'll use a library like PyPDF2 to extract text from the PDF brochure. This will involve:\n",
      "\t* Converting the PDF to text\n",
      "\t* Tokenizing the text\n",
      "\t* Removing stop words and punctuation\n",
      "\t* Lemmatizing the tokens\n",
      "\t* Creating a dictionary to map question tokens to answer IDs\n",
      "\n",
      "**FAQ Database:**\n",
      "\n",
      "1. **CSV-Based FAQs:** We'll store the preprocessed CSV data in a database, such as a SQLite database.\n",
      "2. **PDF-Based FAQs:** We'll store the preprocessed PDF data in a separate database, such as a MongoDB database.\n",
      "\n",
      "**FAQ Bot:**\n",
      "\n",
      "1. **User Input:** The bot will accept user input in the form of questions or keywords related to college life.\n",
      "2. **Question Matching:** The bot will match the user's input with the FAQs in the database.\n",
      "3. **Answer Retrieval:** The bot will retrieve the corresponding answer from the database.\n",
      "4. **Response Generation:** The bot will generate a response to the user, including the answer and any relevant additional information.\n",
      "\n",
      "**Code Example:**\n",
      "\n",
      "Here's an updated example of how the bot might be implemented using Python and the pandas and PyPDF2 libraries:\n",
      "```python\n",
      "import pandas as pd\n",
      "import PyPDF2\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "# Load CSV data\n",
      "csv_data = pd.read_csv('qanda.csv')\n",
      "\n",
      "# Preprocess CSV data\n",
      "csv_data['question'] = csv_data['question'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
      "csv_data['answer'] = csv_data['answer'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
      "csv_data['question'] = csv_data['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
      "csv_data['answer'] = csv_data['answer'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
      "csv_data['question'] = csv_data['question'].apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(word) for word in x.split()]))\n",
      "\n",
      "# Create dictionary to map question tokens to answer IDs\n",
      "question_dict = {}\n",
      "for index, row in csv_data.iterrows():\n",
      "    question_tokens = row['question'].split()\n",
      "    for token in question_tokens:\n",
      "        if token not in question_dict:\n",
      "            question_dict[token] = []\n",
      "        question_dict[token].append(row['answer_id'])\n",
      "\n",
      "# Load PDF data\n",
      "pdf_data = PyPDF2.PdfFileReader('brochure.pdf')\n",
      "text = ''\n",
      "for page in pdf_data.pages:\n",
      "    text += page.extract_text()\n",
      "\n",
      "# Preprocess PDF data\n",
      "pdf_data = text.split('.')\n",
      "pdf_data = [line.strip() for line in pdf_data]\n",
      "pdf_data = [line for line in pdf_data if line]\n",
      "pdf_data = [' '.join(word_tokenize(line)) for line in pdf_data]\n",
      "pdf_data = [' '.join([word for word in line.split() if word not in stopwords.words('english')]) for line in pdf_data]\n",
      "pdf_data = [' '.join([WordNetLemmatizer().lemmatize(word) for word in line.split()]) for line in pdf_data]\n",
      "\n",
      "# Create dictionary to map question tokens to answer IDs\n",
      "pdf_dict = {}\n",
      "for line in pdf_data:\n",
      "    question_tokens = line.split()\n",
      "    for token in question_tokens:\n",
      "        if token not in pdf_dict:\n",
      "            pdf_dict[token] = []\n",
      "        pdf_dict[token].append(line)\n",
      "\n",
      "# Define function to match user input with FAQs\n",
      "def match_input(user_input):\n",
      "    user_input_tokens = word_tokenize(user_input)\n",
      "    user_input_tokens = [word for word in user_input_tokens if word not in stopwords.words('english')]\n",
      "    user_input_tokens = [WordNetLemmatizer().lemmatize(word) for word in user_input_tokens]\n",
      "    for token in user_input_tokens:\n",
      "        if token in question_dict:\n",
      "            return question_dict[token]\n",
      "        elif token in pdf_dict:\n",
      "            return pdf_dict[token]\n",
      "    return 'Sorry, I couldn\\'t find an answer to your question.'\n",
      "\n",
      "# Define function to generate response\n",
      "def generate_response(user_input):\n",
      "    answer = match_input(user_input)\n",
      "    return f'Q: {user_input}\\nA: {answer}'\n",
      "\n",
      "# Test the bot\n",
      "user_input = 'What are the office hours for the registrar\\'s office?'\n",
      "print(generate_response(user_input))\n",
      "```\n",
      "This is just an updated example, and there are many ways to improve the bot's functionality and accuracy.\n",
      "\n",
      "--- User turn 3 ---\n",
      "Assistant: Here's a plan to build a simple web application with a minimal tech stack:\n",
      "\n",
      "**Project:** Simple Blogging Platform\n",
      "\n",
      "**Tech Stack:**\n",
      "\n",
      "1. Frontend: HTML, CSS, JavaScript\n",
      "2. Backend: Node.js with Express.js\n",
      "3. Database: SQLite\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "**Step 1: Set up the project structure**\n",
      "\n",
      "1. Create a new directory for the project and navigate into it.\n",
      "2. Create the following subdirectories:\n",
      "\t* `public`: for static files (e.g., images, CSS, JavaScript)\n",
      "\t* `src`: for source code (e.g., HTML, CSS, JavaScript)\n",
      "\t* `db`: for database files (e.g., SQLite database)\n",
      "3. Create a new file `index.js` in the project root to serve as the entry point.\n",
      "\n",
      "**Step 2: Set up the backend**\n",
      "\n",
      "1. Install Node.js and Express.js using npm:\n",
      "\t* `npm install express`\n",
      "2. Create a new file `server.js` in the `src` directory to define the Express.js server.\n",
      "3. Import the Express.js module and create a new instance:\n",
      "\t* `const express = require('express');`\n",
      "\t* `const app = express();`\n",
      "4. Define a route to serve the frontend:\n",
      "\t* `app.get('/', (req, res) => { res.sendFile(__dirname + '/public/index.html'); });`\n",
      "\n",
      "**Step 3: Set up the database**\n",
      "\n",
      "1. Install the SQLite module using npm:\n",
      "\t* `npm install sqlite3`\n",
      "2. Create a new file `db.js` in the `src` directory to define the SQLite database.\n",
      "3. Import the SQLite module and create a new database connection:\n",
      "\t* `const sqlite3 = require('sqlite3').verbose();`\n",
      "\t* `const db = new sqlite3.Database('./db/blog.db');`\n",
      "4. Create a table to store blog posts:\n",
      "\t* `db.run('CREATE TABLE IF NOT EXISTS posts (id INTEGER PRIMARY KEY, title TEXT, content TEXT)');`\n",
      "\n",
      "**Step 4: Create the frontend**\n",
      "\n",
      "1. Create a new file `index.html` in the `public` directory to define the frontend.\n",
      "2. Use HTML, CSS, and JavaScript to create a simple blog post editor and viewer.\n",
      "\n",
      "**Step 5: Integrate the frontend and backend**\n",
      "\n",
      "1. Update the `server.js` file to serve the frontend:\n",
      "\t* `app.use(express.static(__dirname + '/public'));`\n",
      "2. Create a new route to handle form submissions:\n",
      "\t* `app.post('/posts', (req, res) => { // handle form submission });`\n",
      "\n",
      "**Step 6: Implement form submission handling**\n",
      "\n",
      "1. Update the `server.js` file to handle form submissions:\n",
      "\t* `app.post('/posts', (req, res) => { const title = req.body.title; const content = req.body.content; db.run('INSERT INTO posts (title, content) VALUES (?, ?)', title, content, (err) => { if (err) { console.error(err); } else { res.redirect('/'); } }); });`\n",
      "\n",
      "**Step 7: Test the application**\n",
      "\n",
      "1. Start the server using `node server.js`.\n",
      "2. Open a web browser and navigate to `http://localhost:3000`.\n",
      "3. Test the form submission by entering some data and submitting the form.\n",
      "\n",
      "This plan should give you a basic blogging platform with a minimal tech stack. You can expand on this plan by adding more features, such as user authentication, comments, and image uploads.\n",
      "\n",
      "--- User turn 4 ---\n",
      "Assistant: Here's an updated plan with a quick timeline for a 2-week delivery:\n",
      "\n",
      "**Project:** Simple Blogging Platform\n",
      "\n",
      "**Tech Stack:**\n",
      "\n",
      "1. Frontend: HTML, CSS, JavaScript\n",
      "2. Backend: Node.js with Express.js\n",
      "3. Database: SQLite\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "**Week 1:**\n",
      "\n",
      "**Day 1-2: Set up the project structure and backend**\n",
      "\n",
      "1. Create a new directory for the project and navigate into it.\n",
      "2. Create the following subdirectories:\n",
      "\t* `public`: for static files (e.g., images, CSS, JavaScript)\n",
      "\t* `src`: for source code (e.g., HTML, CSS, JavaScript)\n",
      "\t* `db`: for database files (e.g., SQLite database)\n",
      "3. Create a new file `index.js` in the project root to serve as the entry point.\n",
      "4. Install Node.js and Express.js using npm:\n",
      "\t* `npm install express`\n",
      "5. Create a new file `server.js` in the `src` directory to define the Express.js server.\n",
      "6. Import the Express.js module and create a new instance:\n",
      "\t* `const express = require('express');`\n",
      "\t* `const app = express();`\n",
      "7. Define a route to serve the frontend:\n",
      "\t* `app.get('/', (req, res) => { res.sendFile(__dirname + '/public/index.html'); });`\n",
      "\n",
      "**Day 3-4: Set up the database**\n",
      "\n",
      "1. Install the SQLite module using npm:\n",
      "\t* `npm install sqlite3`\n",
      "2. Create a new file `db.js` in the `src` directory to define the SQLite database.\n",
      "3. Import the SQLite module and create a new database connection:\n",
      "\t* `const sqlite3 = require('sqlite3').verbose();`\n",
      "\t* `const db = new sqlite3.Database('./db/blog.db');`\n",
      "4. Create a table to store blog posts:\n",
      "\t* `db.run('CREATE TABLE IF NOT EXISTS posts (id INTEGER PRIMARY KEY, title TEXT, content TEXT)');`\n",
      "\n",
      "**Day 5-6: Implement form submission handling**\n",
      "\n",
      "1. Update the `server.js` file to handle form submissions:\n",
      "\t* `app.post('/posts', (req, res) => { // handle form submission });`\n",
      "2. Implement form submission handling:\n",
      "\t* `app.post('/posts', (req, res) => { const title = req.body.title; const content = req.body.content; db.run('INSERT INTO posts (title, content) VALUES (?, ?)', title, content, (err) => { if (err) { console.error(err); } else { res.redirect('/'); } }); });`\n",
      "\n",
      "**Week 2:**\n",
      "\n",
      "**Day 7-8: Create the frontend**\n",
      "\n",
      "1. Create a new file `index.html` in the `public` directory to define the frontend.\n",
      "2. Use HTML, CSS, and JavaScript to create a simple blog post editor and viewer.\n",
      "\n",
      "**Day 9-10: Integrate the frontend and backend**\n",
      "\n",
      "1. Update the `server.js` file to serve the frontend:\n",
      "\t* `app.use(express.static(__dirname + '/public'));`\n",
      "2. Test the application by starting the server and submitting a form.\n",
      "\n",
      "**Day 11-12: Test and refine the application**\n",
      "\n",
      "1. Test the application by submitting multiple forms and verifying that the data is stored in the database.\n",
      "2. Refine the application by adding error handling and improving the user experience.\n",
      "\n",
      "**Day 13-14: Finalize the application**\n",
      "\n",
      "1. Review the application and ensure that it meets the requirements.\n",
      "2. Make any final changes and improvements.\n",
      "3. Deliver the application.\n",
      "\n",
      "This timeline assumes that you will work on the project for 2 weeks, with 2 days of setup and backend implementation in the first week, and 2 days of frontend implementation and integration in the second week. The remaining days will be spent testing and refining the application.\n",
      "\n",
      "=== Final conversation history (roles only) ===\n",
      "['user', 'assistant', 'user', 'assistant', 'user', 'assistant', 'user', 'assistant']\n",
      "\n",
      "=== Final conversation history (preview) ===\n",
      "user: We need to design a small FAQ bot for a college site.\n",
      "assistant: Here's a basic design for a small FAQ bot for a college site:\n",
      "\n",
      "**Bot Name:** CollegePal\n",
      "\n",
      "**Functionality:**\n",
      "\n",
      "1. **User Input:** The bot will...\n",
      "user: Data sources are a CSV of Q&A and a PDF brochure.\n",
      "assistant: With a CSV of Q&A and a PDF brochure, we can design a more robust FAQ bot. Here's an updated design:\n",
      "\n",
      "**Data Sources:**\n",
      "\n",
      "1. **CSV of Q&A:** ...\n",
      "user: Suggest a plan with steps and a minimal tech stack.\n",
      "assistant: Here's a plan to build a simple web application with a minimal tech stack:\n",
      "\n",
      "**Project:** Simple Blogging Platform\n",
      "\n",
      "**Tech Stack:**\n",
      "\n",
      "1. Front...\n",
      "user: Add a quick timeline for a 2-week delivery.\n",
      "assistant: Here's an updated plan with a quick timeline for a 2-week delivery:\n",
      "\n",
      "**Project:** Simple Blogging Platform\n",
      "\n",
      "**Tech Stack:**\n",
      "\n",
      "1. Frontend: HT...\n"
     ]
    }
   ],
   "source": [
    "conversation_history.clear()\n",
    "turn_count = 0\n",
    "\n",
    "model_id = \"llama-3.1-8b-instant\"  # or \"llama-3.3-70b-versatile\"\n",
    "\n",
    "inputs = [\n",
    "    \"We need to design a small FAQ bot for a college site.\",\n",
    "    \"Data sources are a CSV of Q&A and a PDF brochure.\",\n",
    "    \"Suggest a plan with steps and a minimal tech stack.\",\n",
    "    \"Add a quick timeline for a 2-week delivery.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(inputs, start=1):\n",
    "    print(f\"\\n--- User turn {i} ---\")\n",
    "    reply = send_chat_with_periodic_summary(\n",
    "        client=client,\n",
    "        model=model_id,\n",
    "        user_text=text,\n",
    "        k=3,            # summarize after every 3rd user turn\n",
    "        keep_last=6,    # keep recent context verbatim\n",
    "        last_n=12,      # cap number of messages sent\n",
    "        max_chars=4000, # cap total characters sent\n",
    "        summary_max_words=80,\n",
    "    )\n",
    "    print(\"Assistant:\", reply)\n",
    "\n",
    "print(\"\\n=== Final conversation history (roles only) ===\")\n",
    "print([m[\"role\"] for m in conversation_history])\n",
    "\n",
    "print(\"\\n=== Final conversation history (preview) ===\")\n",
    "for m in conversation_history:\n",
    "    print(f\"{m['role']}: {m['content'][:140]}{'...' if len(m['content'])>140 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757758248617,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "Nb_YqzUzf6pZ"
   },
   "outputs": [],
   "source": [
    "extract_profile_tool = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_profile\",\n",
    "        \"description\": \"Extract a simple profile from free text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Full name if present; otherwise empty string.\"\n",
    "                },\n",
    "                \"email\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Email address if present; otherwise empty string.\"\n",
    "                },\n",
    "                \"phone\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Phone number if present; include country/area code if available.\"\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and state/region/country if present; otherwise empty string.\"\n",
    "                },\n",
    "                \"age\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Age in years if present; otherwise 0.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1757758295889,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "TTV27Tqaf6oe"
   },
   "outputs": [],
   "source": [
    "def run_extraction_with_tools(client, model: str, user_text: str):\n",
    "    \"\"\"\n",
    "    Sends user_text with the extract_profile_tool available.\n",
    "    Returns the raw response so the caller can parse tool_calls.\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,  # e.g., \"llama-3.1-8b-instant\" or \"llama-3.3-70b-versatile\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"When relevant, extract profile fields via the provided function.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "        tools=extract_profile_tool,        # from Step 5a\n",
    "        tool_choice=\"auto\",               # let the model decide\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1757758404071,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "dVJqjRuijmqr"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def parse_extraction_response(resp):\n",
    "    \"\"\"\n",
    "    Extracts the first tool call's JSON arguments if present,\n",
    "    returns a dict with fields: name, email, phone, location, age.\n",
    "    Falls back to empty/defaults if no tool call was made.\n",
    "    \"\"\"\n",
    "    result = {\"name\": \"\", \"email\": \"\", \"phone\": \"\", \"location\": \"\", \"age\": 0}\n",
    "\n",
    "    if not getattr(resp, \"choices\", None):\n",
    "        return result\n",
    "\n",
    "    choice0 = resp.choices[0] # Access the first choice\n",
    "    tool_calls = getattr(choice0.message, \"tool_calls\", None)\n",
    "\n",
    "    if not tool_calls:\n",
    "        return result\n",
    "    call = tool_calls[0]\n",
    "    fn = getattr(call, \"function\", None)\n",
    "    if not fn or not getattr(fn, \"arguments\", None):\n",
    "        return result\n",
    "\n",
    "    # Parse JSON arguments string into a dict\n",
    "    try:\n",
    "        args = json.loads(fn.arguments)\n",
    "    except Exception:\n",
    "        return result\n",
    "\n",
    "    name = str(args.get(\"name\", \"\") or \"\")\n",
    "    email = str(args.get(\"email\", \"\") or \"\")\n",
    "    phone = str(args.get(\"phone\", \"\") or \"\")\n",
    "    location = str(args.get(\"location\", \"\") or \"\")\n",
    "\n",
    "    age_raw = args.get(\"age\", 0)\n",
    "    try:\n",
    "        age = int(age_raw)\n",
    "    except Exception:\n",
    "        age = 0\n",
    "\n",
    "    result.update({\"name\": name, \"email\": email, \"phone\": phone, \"location\": location, \"age\": age})\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1757758405789,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "tkHGscfwjmm7",
    "outputId": "dd1d84cc-cf8c-4d52-e621-58b4a0973727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Input: I'm Riya Patel, 21, from Ahmedabad. Email: riya.p@example.com, phone +91 98765 43210.\n",
      "Parsed: {'name': 'Riya Patel', 'email': 'riya.p@example.com', 'phone': '+91 98765 43210', 'location': 'Ahmedabad', 'age': 21}\n",
      "\n",
      "--- Sample 2 ---\n",
      "Input: This is Arjun M., based in Surat. Contact me at arjun.m@demo.in. Age twenty is fine. Call 079-22223333.\n",
      "Parsed: {'name': 'Arjun M.', 'email': 'arjun.m@demo.in', 'phone': '079-22223333', 'location': 'Surat', 'age': 20}\n",
      "\n",
      "--- Sample 3 ---\n",
      "Input: Name: (unknown). Reach at support@example.org, age 34, location Rajkot, phone missing.\n",
      "Parsed: {'name': '(unknown)', 'email': 'support@example.org', 'phone': '', 'location': 'Rajkot', 'age': 34}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = [\n",
    "    \"I'm Riya Patel, 21, from Ahmedabad. Email: riya.p@example.com, phone +91 98765 43210.\",\n",
    "    \"This is Arjun M., based in Surat. Contact me at arjun.m@demo.in. Age twenty is fine. Call 079-22223333.\",\n",
    "    \"Name: (unknown). Reach at support@example.org, age 34, location Rajkot, phone missing.\",\n",
    "]\n",
    "\n",
    "model_id = \"llama-3.1-8b-instant\"  # or \"llama-3.3-70b-versatile\"\n",
    "\n",
    "for i, text in enumerate(samples, start=1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    raw = run_extraction_with_tools(client, model_id, text)\n",
    "    data = parse_extraction_response(raw)\n",
    "    print(\"Input:\", text)\n",
    "    print(\"Parsed:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1757758616219,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "2eoRTuXojmlw"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def extract_with_json_mode(client, model: str, user_text: str):\n",
    "    \"\"\"\n",
    "    Forces the model to return strict JSON conforming to the schema.\n",
    "    No function-calling/tools are used here.\n",
    "    \"\"\"\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"phone\": {\"type\": \"string\"},\n",
    "            \"location\": {\"type\": \"string\"},\n",
    "            \"age\": {\"type\": \"integer\"}\n",
    "        },\n",
    "        \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the requested fields and return ONLY valid JSON matching the provided schema.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "        response_format={  # Groq supports OpenAI-compatible structured outputs\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"profile_schema\",\n",
    "                \"schema\": schema,\n",
    "                \"strict\": True  # enforce strict conformance\n",
    "            }\n",
    "        },\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    # The text is guaranteed to be valid JSON under strict mode\n",
    "    raw_json = resp.choices.message.content\n",
    "    data = json.loads(raw_json)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1757758775492,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "vqKRu9RvjmgV"
   },
   "outputs": [],
   "source": [
    "def extract_with_json_mode(client, model: str, user_text: str):\n",
    "    import json\n",
    "    schema_hint = \"Return ONLY a JSON object with keys: name, email, phone, location, age. Age must be an integer.\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": schema_hint},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    raw = resp.choices[0].message.content # Access the message content from the first choice\n",
    "    data = json.loads(raw)\n",
    "    # coerce/validate fields as before\n",
    "    data.setdefault(\"name\",\"\"); data.setdefault(\"email\",\"\"); data.setdefault(\"phone\",\"\"); data.setdefault(\"location\",\"\");\n",
    "    try: data[\"age\"] = int(data.get(\"age\", 0))\n",
    "    except Exception: data[\"age\"] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1757758776947,
     "user": {
      "displayName": "goku k",
      "userId": "01088663728470404813"
     },
     "user_tz": -330
    },
    "id": "VQR5MqXSjmjO",
    "outputId": "debd3154-720e-4653-922d-0231fa4dd5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Input ===\n",
      "Hi, I'm Riya Patel, 21, based in Ahmedabad. Email: riya.p@example.com, phone +91-98765-43210.\n",
      "\n",
      "--- Function Calling (tools) ---\n",
      "{'name': '', 'email': '', 'phone': '', 'location': '', 'age': 0}\n",
      "\n",
      "--- Strict JSON Mode ---\n",
      "{'name': 'Riya Patel', 'email': 'riya.p@example.com', 'phone': '+91-98765-43210', 'location': 'Ahmedabad', 'age': 21}\n"
     ]
    }
   ],
   "source": [
    "def compare_extraction_paths(client, model: str, text: str):\n",
    "    print(\"\\n=== Input ===\")\n",
    "    print(text)\n",
    "\n",
    "    # Path 1: Function calling (tools)\n",
    "    resp_tools = run_extraction_with_tools(client, model, text)\n",
    "    parsed_tools = parse_extraction_response(resp_tools)\n",
    "\n",
    "    # Path 2: Strict JSON mode (no tools)\n",
    "    parsed_json_mode = extract_with_json_mode(client, model, text)  # Option A1\n",
    "\n",
    "    print(\"\\n--- Function Calling (tools) ---\")\n",
    "    print(parsed_tools)\n",
    "\n",
    "    print(\"\\n--- Strict JSON Mode ---\")\n",
    "    print(parsed_json_mode)\n",
    "\n",
    "# --- Example usage ---\n",
    "sample = \"Hi, I'm Riya Patel, 21, based in Ahmedabad. Email: riya.p@example.com, phone +91-98765-43210.\"\n",
    "compare_extraction_paths(client, \"llama-3.1-8b-instant\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UE19V7jjmea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO2nZwVfjmcD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWdd608-jmZb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYZTbYfjf6lv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQKVeAiLToL4MVBekVDBzQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
